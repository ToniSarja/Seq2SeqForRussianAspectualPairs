{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada4b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac6800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['верить',\t\n",
    "'говорить',\n",
    "'держать',\t\n",
    "'думать',\n",
    "'желать',\n",
    "'играть',\n",
    "'искать',\n",
    "'курить',\n",
    "'лежать',\n",
    "'просить',\n",
    "'сидеть',\n",
    "'слушать',\n",
    "'смотреть',\n",
    "'ставить',\n",
    "'стоять',\n",
    "'строить',\n",
    "'считать',\n",
    "'терять',\n",
    "'тратить']\n",
    "\n",
    "prime = ['поверить',\n",
    "'поговорить',\n",
    "'подержать',\n",
    "'подумать',\n",
    "'пожелать',\n",
    "'поиграть',\n",
    "'поискать',\n",
    "'покурить',\n",
    "'полежать',\n",
    "'попросить',\n",
    "'посидеть',\n",
    "'послушать',\n",
    "'посмотреть',\n",
    "'поставить',\n",
    "'постоять',\n",
    "'построить',\n",
    "'посчитать',\n",
    "'потерять',\n",
    "'потратить']\n",
    "\n",
    "pairs = list(zip(target, prime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b046ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('верить', 'поверить'),\n",
       " ('говорить', 'поговорить'),\n",
       " ('держать', 'подержать'),\n",
       " ('думать', 'подумать'),\n",
       " ('желать', 'пожелать'),\n",
       " ('играть', 'поиграть'),\n",
       " ('искать', 'поискать'),\n",
       " ('курить', 'покурить'),\n",
       " ('лежать', 'полежать'),\n",
       " ('просить', 'попросить'),\n",
       " ('сидеть', 'посидеть'),\n",
       " ('слушать', 'послушать'),\n",
       " ('смотреть', 'посмотреть'),\n",
       " ('ставить', 'поставить'),\n",
       " ('стоять', 'постоять'),\n",
       " ('строить', 'построить'),\n",
       " ('считать', 'посчитать'),\n",
       " ('терять', 'потерять'),\n",
       " ('тратить', 'потратить')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca4094ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = [char for src, tgt in pairs for char in src + tgt]\n",
    "chars = sorted(set(all_text))\n",
    "char2idx = {c: i+4 for i, c in enumerate(chars)}\n",
    "char2idx[\"<PAD>\"] = 0\n",
    "char2idx[\"<SOS>\"] = 1\n",
    "char2idx[\"<EOS>\"] = 2\n",
    "char2idx[\"<UNK>\"] = 3\n",
    "idx2char = {i: c for c, i in char2idx.items()}\n",
    "vocab_size = len(char2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e79aed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerbPairDataset(Dataset):\n",
    "    def __init__(self, pairs, char2idx, max_len=20):\n",
    "        self.pairs = pairs\n",
    "        self.char2idx = char2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def encode(self, word, add_sos=False):\n",
    "        seq = [char2idx.get(c, char2idx[\"<UNK>\"]) for c in word]\n",
    "        if add_sos:\n",
    "            seq = [char2idx[\"<SOS>\"]] + seq\n",
    "        seq = seq + [char2idx[\"<EOS>\"]]\n",
    "        seq += [char2idx[\"<PAD>\"]] * (self.max_len - len(seq))\n",
    "        return seq[:self.max_len]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.pairs[idx]\n",
    "        src_encoded = self.encode(src)\n",
    "        tgt_encoded = self.encode(tgt, add_sos=True)\n",
    "        return torch.tensor(src_encoded), torch.tensor(tgt_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6ad12aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = x.unsqueeze(1)\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44e4333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(encoder, decoder, dataloader, epochs=20, teacher_forcing_ratio=0.5):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=char2idx[\"<PAD>\"])\n",
    "    enc_opt = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "    dec_opt = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for src, tgt in dataloader:\n",
    "            enc_opt.zero_grad()\n",
    "            dec_opt.zero_grad()\n",
    "            hidden, cell = encoder(src)\n",
    "            input_token = tgt[:, 0]\n",
    "            loss = 0\n",
    "            for t in range(1, tgt.shape[1]):\n",
    "                output, hidden, cell = decoder(input_token, hidden, cell)\n",
    "                loss += criterion(output, tgt[:, t])\n",
    "                teacher_force = random.random() < teacher_forcing_ratio\n",
    "                top1 = output.argmax(1)\n",
    "                input_token = tgt[:, t] if teacher_force else top1\n",
    "            loss.backward()\n",
    "            enc_opt.step()\n",
    "            dec_opt.step()\n",
    "            total_loss += loss.item() / tgt.shape[1]\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73aca8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoder, decoder, word, max_len=20):\n",
    "    with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        input_seq = torch.tensor([dataset.encode(word)], dtype=torch.long)\n",
    "        hidden, cell = encoder(input_seq)\n",
    "        input_token = torch.tensor([char2idx[\"<SOS>\"]])\n",
    "        output_seq = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            output, hidden, cell = decoder(input_token, hidden, cell)\n",
    "            top1 = output.argmax(1)\n",
    "            if top1.item() == char2idx[\"<EOS>\"]:\n",
    "                break\n",
    "            output_seq.append(idx2char[top1.item()])\n",
    "            input_token = top1\n",
    "\n",
    "        return \"\".join(output_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f99da781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n"
     ]
    }
   ],
   "source": [
    "dataset = VerbPairDataset(pairs, char2idx)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "encoder = Encoder(vocab_size, emb_dim=64, hidden_dim=128)\n",
    "decoder = Decoder(vocab_size, emb_dim=64, hidden_dim=128)\n",
    "\n",
    "train_seq2seq(encoder, decoder, dataloader, epochs=30)\n",
    "\n",
    "# Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "740e02d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "посерать\n",
      "посерать\n",
      "пострать\n",
      "пострать\n",
      "пострать\n",
      "посерать\n"
     ]
    }
   ],
   "source": [
    "print(predict(encoder, decoder, \"думать\"))\n",
    "print(predict(encoder, decoder, \"желать\"))\n",
    "print(predict(encoder, decoder, \"строить\"))\n",
    "print(predict(encoder, decoder, \"говорить\"))\n",
    "print(predict(encoder, decoder, \"строить\"))\n",
    "print(predict(encoder, decoder, \"считать\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7da94adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match_accuracy(predictions, targets):\n",
    "    correct = sum([pred == tgt for pred, tgt in zip(predictions, targets)])\n",
    "    return correct / len(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43cd0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_accuracy(predictions, targets):\n",
    "    total_chars = 0\n",
    "    correct_chars = 0\n",
    "    for pred, tgt in zip(predictions, targets):\n",
    "        for pc, tc in zip(pred, tgt):\n",
    "            total_chars += 1\n",
    "            if pc == tc:\n",
    "                correct_chars += 1\n",
    "    return correct_chars / total_chars if total_chars > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e776ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "\n",
    "def average_edit_distance(predictions, targets):\n",
    "    total_distance = 0\n",
    "    for pred, tgt in zip(predictions, targets):\n",
    "        total_distance += editdistance.eval(pred, tgt)\n",
    "    return total_distance / len(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8e860f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def average_bleu(predictions, targets):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    scores = [\n",
    "        sentence_bleu([list(tgt)], list(pred), weights=(1.0, 0, 0, 0), smoothing_function=smoothie)\n",
    "        for pred, tgt in zip(predictions, targets)\n",
    "    ]\n",
    "    return sum(scores) / len(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "121110b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match Accuracy: 0.6666666666666666\n",
      "Character Accuracy: 0.8461538461538461\n",
      "Average Edit Distance: 1.3333333333333333\n",
      "BLEU Score: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "predictions = [\"поговорить\", \"поиграть\", \"покурить\"]\n",
    "targets = [\"поговорить\", \"поиграть\", \"полежать\"]\n",
    "\n",
    "print(\"Exact Match Accuracy:\", exact_match_accuracy(predictions, targets))\n",
    "print(\"Character Accuracy:\", character_accuracy(predictions, targets))\n",
    "print(\"Average Edit Distance:\", average_edit_distance(predictions, targets))\n",
    "print(\"BLEU Score:\", average_bleu(predictions, targets))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_stuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
